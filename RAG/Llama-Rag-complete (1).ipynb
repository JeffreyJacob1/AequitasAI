{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0b2e20e",
   "metadata": {},
   "source": [
    "# Load in model\n",
    "\n",
    "first load in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b820b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/global/scratch/users/lgoldnercohentzedek/.conda/envs/llm-docs/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 4 CUDA devices:\n",
      "  Device 0: Tesla K80, compute capability 3.7, VMM: yes\n",
      "  Device 1: Tesla K80, compute capability 3.7, VMM: yes\n",
      "  Device 2: Tesla K80, compute capability 3.7, VMM: yes\n",
      "  Device 3: Tesla K80, compute capability 3.7, VMM: yes\n",
      "llama_model_loader: loaded meta data with 16 key-value pairs and 291 tensors from /global/scratch/users/lgoldnercohentzedek/llm-docs/models/7B/ggml-model-q4_0.bin (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = llama\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 2\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_0:  225 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 2048\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 2048\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_0\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n",
      "llm_load_print_meta: general.name     = llama\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.56 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =   977.35 MiB\n",
      "llm_load_tensors:      CUDA1 buffer size =   868.75 MiB\n",
      "llm_load_tensors:      CUDA2 buffer size =   868.75 MiB\n",
      "llm_load_tensors:      CUDA3 buffer size =   862.71 MiB\n",
      "..................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 3900\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   548.44 MiB\n",
      "llama_kv_cache_init:      CUDA1 KV buffer size =   487.50 MiB\n",
      "llama_kv_cache_init:      CUDA2 KV buffer size =   487.50 MiB\n",
      "llama_kv_cache_init:      CUDA3 KV buffer size =   426.56 MiB\n",
      "llama_new_context_with_model: KV self size  = 1950.00 MiB, K (f16):  975.00 MiB, V (f16):  975.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host input buffer size   =    15.64 MiB\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   302.91 MiB\n",
      "llama_new_context_with_model:      CUDA1 compute buffer size =   302.91 MiB\n",
      "llama_new_context_with_model:      CUDA2 compute buffer size =   302.91 MiB\n",
      "llama_new_context_with_model:      CUDA3 compute buffer size =   302.91 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =     8.80 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 9\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'general.file_type': '2', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'general.name': 'llama', 'llama.block_count': '32', 'llama.context_length': '2048', 'general.architecture': 'llama'}\n",
      "config.json: 100%|██████████| 743/743 [00:00<00:00, 116kB/s]\n",
      "model.safetensors: 100%|██████████| 133M/133M [00:01<00:00, 108MB/s]  \n",
      "tokenizer_config.json: 100%|██████████| 366/366 [00:00<00:00, 68.8kB/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.96MB/s]\n",
      "tokenizer.json: 100%|██████████| 711k/711k [00:00<00:00, 3.95MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 125/125 [00:00<00:00, 44.1kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs Loaded!\n",
      "Index Created!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "from llama_index.callbacks import CallbackManager, LlamaDebugHandler\n",
    "from llama_index.llms import LlamaCPP\n",
    "from llama_index.llms.llama_utils import messages_to_prompt, completion_to_prompt\n",
    "\n",
    "from llama_index import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index import ServiceContext\n",
    "from llama_index import set_global_tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "from llama_index.embeddings import HuggingFaceEmbedding\n",
    "\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)  # Change INFO to DEBUG if you want more extensive logging\n",
    "logging.getLogger().addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "llama_debug = LlamaDebugHandler(print_trace_on_end=True)\n",
    "callback_manager = CallbackManager([llama_debug])\n",
    "\n",
    "llm = LlamaCPP(\n",
    "    \n",
    "    # optionally, you can set the path to a pre-downloaded model instead of model_url\n",
    "    model_path=\"/global/scratch/users/lgoldnercohentzedek/llm-docs/models/7B/ggml-model-q4_0.bin\",\n",
    "    \n",
    "    temperature=0.1,\n",
    "    max_new_tokens=256,\n",
    "    \n",
    "    # llama2 has a context window of 4096 tokens, but we set it lower to allow for some wiggle room\n",
    "    context_window=3900,  # note, this sets n_ctx in the model_kwargs below, so you don't need to pass it there.\n",
    "    \n",
    "    # kwargs to pass to __call__()\n",
    "    generate_kwargs={},\n",
    "    \n",
    "    # kwargs to pass to __init__()\n",
    "    # set to at least 1 to use GPU\n",
    "    model_kwargs={\"n_gpu_layers\": 40}, # I need to play with this and see if it actually helps\n",
    "    \n",
    "    # transform inputs into Llama2 format\n",
    "    messages_to_prompt=messages_to_prompt,\n",
    "    completion_to_prompt=completion_to_prompt,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "# create a service context\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    chunk_size=512\n",
    ")\n",
    "\n",
    "documents = SimpleDirectoryReader(\"/global/scratch/users/lgoldnercohentzedek/llm-docs/flat-rit-docs\").load_data()\n",
    "\n",
    "print(\"Docs Loaded!\")\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")\n",
    "print(\"Index Created!\")\n",
    "# set up query engine\n",
    "query_engine = index.as_query_engine(similarity_top_k=2)\n",
    "\n",
    "#response = query_engine.query(\"What is Savio?\")\n",
    "#print(response)\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ae4eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b544c10b",
   "metadata": {},
   "source": [
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba970e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcdf1a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b7ff7cb",
   "metadata": {},
   "source": [
    "# create pipeline for retrieving source text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1cd01d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    Response,\n",
    ")\n",
    "from llama_index.evaluation import FaithfulnessEvaluator\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "evaluator_llama = FaithfulnessEvaluator(service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    chunk_size=512\n",
    "))\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 0)\n",
    "\n",
    "# create vector index\n",
    "splitter = SentenceSplitter(chunk_size=512)\n",
    "vector_index = VectorStoreIndex.from_documents(\n",
    "    documents, service_context=service_context\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "from llama_index.evaluation import EvaluationResult\n",
    "\n",
    "\n",
    "# define jupyter display function\n",
    "def display_eval_df(response: Response, eval_result: EvaluationResult) -> None:\n",
    "    if response.source_nodes == []:\n",
    "        print(\"no response!\")\n",
    "        return\n",
    "    eval_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Response\": str(response),\n",
    "            \"Source\": response.source_nodes[0].node.text[:1000] + \"...\",\n",
    "            \"Evaluation Result\": \"Pass\" if eval_result.passing else \"Fail\",\n",
    "            \"Reasoning\": eval_result.feedback,\n",
    "        },\n",
    "        index=[0],\n",
    "    )\n",
    "    eval_df = eval_df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"600px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        },\n",
    "        subset=[\"Response\", \"Source\"]\n",
    "    )\n",
    "    display(eval_df)\n",
    "    \n",
    "query_engine_eval = vector_index.as_query_engine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a04a4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d8381a9",
   "metadata": {},
   "source": [
    "## ask a question and get the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a39bcb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =    4530.43 ms\n",
      "llama_print_timings:      sample time =      45.83 ms /    89 runs   (    0.51 ms per token,  1942.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7379.51 ms /   697 tokens (   10.59 ms per token,    94.45 tokens per second)\n",
      "llama_print_timings:        eval time =   21831.65 ms /    88 runs   (  248.09 ms per token,     4.03 tokens per second)\n",
      "llama_print_timings:       total time =   29480.28 ms /   785 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    4530.43 ms\n",
      "llama_print_timings:      sample time =       1.55 ms /     3 runs   (    0.52 ms per token,  1939.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7491.45 ms /   881 tokens (    8.50 ms per token,   117.60 tokens per second)\n",
      "llama_print_timings:        eval time =     524.79 ms /     2 runs   (  262.39 ms per token,     3.81 tokens per second)\n",
      "llama_print_timings:       total time =    8078.34 ms /   883 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response:   Based on the provided context information, Savio can be used with moderately sensitive data (P2/P3 data) as defined by UC Policy (IS-3) and documented by the campus Information Security Office. Savio is not appropriate for highly sensitive (P4) data, and researchers should consult the Secure Research Data and Compute web page for information on the service that supports working with highly sensitive data.\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def get_response_and_source(response: Response) -> (str, str):\n",
    "    if not response.source_nodes:\n",
    "        return \"No response!\", \"No source available.\"\n",
    "\n",
    "    # Assuming you're interested in the first source node for simplicity\n",
    "    response_text = str(response)\n",
    "    source_text = response.source_nodes[0].node.text[:1000] + \"...\" if len(response.source_nodes[0].node.text) > 1000 else response.source_nodes[0].node.text\n",
    "\n",
    "    return response_text, source_text\n",
    "\n",
    "# Example usage\n",
    "response_vector = query_engine_eval.query(\"what security levels of data can savio be used with?\")\n",
    "eval_result = evaluator_llama.evaluate_response(response=response_vector)\n",
    "\n",
    "response_text, source_text = get_response_and_source(response_vector)\n",
    "print(\"Response:\", response_text)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799c7844",
   "metadata": {},
   "source": [
    "# retreive URL of source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16c365cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched URL: https://docs-research-it.berkeley.edu/services/high-performance-computing/getting-account/sensitive-accounts\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import difflib\n",
    "\n",
    "def calculate_similarity(file_text, source_text):\n",
    "    return difflib.SequenceMatcher(None, file_text, source_text).ratio()\n",
    "\n",
    "def search_markdown_files(root_dir, source_text, starting_similarity_threshold=0.05, threshold_increment=0.01):\n",
    "    current_threshold = starting_similarity_threshold\n",
    "    matched_files = []\n",
    "    \n",
    "    # Continue searching until one or two files are left\n",
    "    while len(matched_files) > 2 or len(matched_files) == 0:\n",
    "        matched_files = []\n",
    "        for subdir, dirs, files in os.walk(root_dir):\n",
    "            for filename in files:\n",
    "                if filename.endswith('.md'):\n",
    "                    filepath = os.path.join(subdir, filename)\n",
    "                    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "                        file_text = file.read()\n",
    "                    similarity = calculate_similarity(file_text, source_text)\n",
    "                    if similarity >= current_threshold:\n",
    "                        matched_files.append((filepath, similarity))\n",
    "                        #print([(path, round(sim, 2)) for path, sim in matched_files])\n",
    "        # If more than two files match, increase the threshold\n",
    "        if len(matched_files) > 2:\n",
    "            current_threshold += threshold_increment\n",
    "\n",
    "    # Sort matched files by similarity, highest first\n",
    "    matched_files.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Construct the URL from the most similar file's path\n",
    "    if matched_files:\n",
    "        matched_file_path = matched_files[0][0]  # File with the highest similarity\n",
    "        # Extract the part of the path after \"services\" and construct the URL\n",
    "        url = \"https://docs-research-it.berkeley.edu/services\" + matched_file_path.split(\"services\", 1)[1].replace(\".md\", \"\")\n",
    "        return url\n",
    "    else:\n",
    "        return \"No matching URL found.\"\n",
    "\n",
    "# Example usage\n",
    "\n",
    "root_directory = '/global/scratch/users/jejacob/rit-docs-main/docs'\n",
    "matched_url = search_markdown_files(root_directory, source_text)\n",
    "print(\"Matched URL:\", matched_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b9a9daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Savio can be used with moderately sensitive data (P2/P3 data) as defined by UC Policy (IS-3) and documented by the campus Information Security Office. Savio is not appropriate for highly sensitive (P4) data, and researchers should consult the Secure Research Data and Compute web page for information on the service that supports working with highly sensitive data. Here is a link to further documentation: https://docs-research-it.berkeley.edu/services/high-performance-computing/getting-account/sensitive-accounts \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\"\"\n",
    "\n",
    "Savio can be used with moderately sensitive data (P2/P3 data) as defined by UC Policy (IS-3) and documented by the campus Information Security Office. Savio is not appropriate for highly sensitive (P4) data, and researchers should consult the Secure Research Data and Compute web page for information on the service that supports working with highly sensitive data. Here is a link to further documentation: https://docs-research-it.berkeley.edu/services/high-performance-computing/getting-account/sensitive-accounts \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "     )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a9cfc24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "72a8fbbe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66f31a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82472c92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0dede960",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1021c5de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73012259",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57a0265",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6d07f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6cd422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-Docs",
   "language": "python",
   "name": "llm-docs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
